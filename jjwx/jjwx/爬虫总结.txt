## 为了尽可能的保障数据的优质性、可靠性,，只爬取作品库里的作品

1. 晋江文学城主要的难点在于，页数大于10时，如page>10，小说内容
必须要登录后才能访问。登录所需要的操作难点在于：登录动作包含了
滑动验证，协议勾选。后一个比较简单，但是滑动验证的图片比较复杂。
需要一定的时间去匹配缺失内容。因此放弃。另一种选择登录方法则是
QQ登录，然而搜遍全网也没能搜索到scrapy相关的插件。遂放弃。

2.最后，模拟登录选择的方案，使用浏览器自动化测试软件包 Selenium，
来模拟登录，由此带来的问题时，爬取时不能异步，多线程处理，因此
爬取速度就会降低32x左右

3.爬取前应该充分的分析出网站结构，分析出陷阱。第一个陷阱在于，如果
你老老实实从网站入口，爬到目标内容所在地，那么你就上当了。该网站的
二级入口是通过javascript加载的，然后返回真正的二级入口，如果不能
破解javascript的处理逻辑，那么便只能获取一个重复的假入口。前期由于
没有发现这个特点，浪费了大量的时间，删除了大量的代码。以至于后面重
写爬虫逻辑

4.第二个陷阱则是，每一个页的总作品数也是随机的，比如第 2 页包含了
50 个作品，但是第 177 页则只有 30 个作品，然而第 178 页又变成了 50
个作品，因此，在用 xpath 解析内容前，应该先获取总的作品数，再用 python
循环语句逐个解析所需内容。

5.第三个陷阱是：在 ’进度‘ 这个字段里，总共有 3 个状态，分别是'完结'
（红色）, '暂停'(蓝色), '连载'(黑色)。不同的颜色有不同的 xpath 网页
结构，红色，蓝色结构一致，但是黑色不一样，因此要写两个不同的解析体来
获取进度内容

6.第四个陷阱是，晋江文学城对每个用户设置了访问限制（猜测的），因为当我
访问了所有类别的前100页时，没有任何问题，但是当我想要继续访问101-页后，
后台说服务器资源用尽。具体恢复时间尚不清楚，有可能是3天更新用户访问资源，
也有可能是一周来更新。为什么能排除User-Agent等问题？使用了随机生成
的User-Agent，但是都不能访问100-页之后的内容。故猜测为用户访问
是由上限的。目前拟定的解决办法，使用另一个绑定手机号的账户来进行测试，看
能否访问到100页之后的内容，准备尝试随机IP

7.第五个陷阱：经过不断尝试后发现，该网站涉及IP ban，需要随时更改 IP 设置，
通过 DownloadMiddleware 中间组件去实现，即每一次在 request 时，从 免费的 
IP 提供服务商请求网址，获取IP池，免费的不太稳定，但是对于这次足够了，设置后
，便能成功访问后100-页的内容了。

8.考虑过用 scrapy-playwright 来加速爬取速度。但是初试 playwright，
并没有找到如何打开有头模式，如果能初始化打开，先完成模拟登录的操作
后，再将 playwright 设置为无头，则可以利用异步优势，加速爬取过程，
由于时间关系，遂放弃。

8.考虑到晋江文学城的结构，将所有作品库的基本信息爬取完成后。通过
生成的 csv 文件，预处理文件提取出所有作者的链接，为了爬取后面作者的
相关信息，比如一共写了多少部小说，年龄（如果有）。作品链接同理。设想
的操作先通过数据处理库 pandas 的 drop_duplicate 方法，从大量的重复
的作者中，筛选出唯一的作者极其链接，然后，再写一个爬虫程序单独爬取作
者的相关信息，最后与作品表进行合并(merge)。作品同理。

9.分析作品后发现，由于一些作品类型的交叉性，如某些作品即使原创，也是
言情。所以在言情作品库中，和原创中作品库可能会出现同样作品。爬取完后，
要进行去重处理。

10.进行无差别爬取时，由于数量过多，采取多线程的方式爬取

ps: 原来 scrapy 中异常次数到达阈值后，会停止运行爬虫程序!